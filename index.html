<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Jon Barron and Saurabh Gupta. Thanks to Jeff Donahue for email scamble.*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    clear:both;
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }

  /* Borrowed from https://siddancha.github.io */
  img.pub {
    padding: 0px;
    margin: 0px;
    /* border-radius:15px; */
    /* border:1px solid black; */
  }

  video.pub {
    padding: 0px;
    margin: 0px;
    /* border-radius:15px; */
    /* border:1px solid black; */
  }

  .paper {
    font-size: 12px;
    margin-left: 30px;
  }

  pre {
    border: solid;
    border-width: 1px;
    padding: 5px;
  }

  tr.pub {
    padding: 0px;
    margin: 0px
  }

  td
  .pub_thumbnail {
    width: 39.1%;
    padding-left: 0px;
    padding-right: 10px;
    padding-top: 20px;
    padding-bottom: 20px;
    margin: 0px;
    text-align: center;
    vertical-align: top;
  }
  .pub_text {
    width: 60.9%;
    padding-left: 0px;
    padding-right: 0px;
    padding-top: 20px;
    padding-bottom: 20px;
    margin: 0px;
    text-align: left;
    vertical-align: top;
  }

  sticker {
    color:white;
    background-color: red;
    font-family: Arial, sans-serif;
    font-size: 13px;
    font-weight: bold;
    vertical-align: center;
    padding-left: 2px;
    padding-right: 2px;
  }

  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="data/images/ri_logo.png">
  <link rel="icon" type="image/png" href="data/images/ri_logo.png">
  <link rel="apple-touch-icon" type="image/png" href="data/images/ri_logo.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Khiem Vuong</title>
  <meta name="Khiem Vuong's Homepage" http-equiv="Content-Type" content="Khiem Vuong's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173597671-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-173597671-1');

    var x = document.lastModified;

  </script>
  
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>

</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Khiem Vuong</font><br> -->
    <pageheading>Khiem Vuong</pageheading><br>
    <b>email</b>:&nbsp kvuong at andrew dot cmu dot edu
    <!-- <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font> -->
    <!-- <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', '@seucmvgudkn.uco.',
        [7,  9, 15,  3,  8, 12,  2,  6, 13, 16,  1,  5, 14, 17, 11,  4, 10]);
    </script> -->
  </p>

  <tr>
    <td width="30%" valign="top"><a href="data/images/portrait.jpg"><img src="data/images/portrait.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    <a href="data/Khiem_resume.pdf">CV</a> |
    <!-- <a href="bio.txt" target="_blank">Bio</a> | -->
    <a href="https://scholar.google.com/citations?hl=en&user=ow6ni6oAAAAJ">Google Scholar</a> |
    <!-- <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-132.pdf" target="_blank">Phd Thesis</a> | -->
    <a href="https://github.com/kvuong2711">Github</a> |
    <!-- <a href="https://twitter.com/pathak2206">Twitter</a> <br/> -->
    <a href="https://www.linkedin.com/in/kvuong2711/">LinkedIn</a>
    </p>
    </td>
    <td width="69%" valign="top" align="justify">
    <p>I'm a graduate student (MS in Robotics) in the <a href="https://www.ri.cmu.edu/" target="_blank">Robotics Institute</a> at <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a>. I'm advised by <a href='http://www.cs.cmu.edu/~srinivas/' target="_blank">Srinivasa Narasimhan</a> as part of the <a href='https://imaging.cs.cmu.edu/' target="_blank">Imaging Group</a> at CMU. 

    <p>
      Previously, I received my B.S. in Computer Science from the <a href="https://cse.umn.edu/cs" target="_blank">University of Minnesota</a>, working with <a href="https://www-users.cs.umn.edu/~stergios/index.html" target="_blank">Stergios Roumeliotis</a> and <a href="https://www-users.cs.umn.edu/~hspark/" target="_blank">Hyun Soo Park</a> on computer vision. 
      I also spent a summer at <a href="https://www.enfusionsystems.com/" target="_blank">Enfusion Systems</a> as a software engineering intern during my sophomore year.</p>
    <p>
      I'm interested in computer vision, robotics, and machine learning. Most of my research is about geometry-based vision, mainly 3D/4D reconstruction, perception, and visual scene understanding in general.
<!--     <p>
      <sticker>NEW</sticker><strong> I'm looking for full-time job opportunities, especially in ML/Vision-related roles, in both products and research teams. Please connect if you think there might be a good fit!</strong>
    </p> -->

    </td>
  </tr>
</table><hr>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="7.5px" style="padding-bottom: 10px;">
  <tr>
    <td width="16%" valign="top" align="right"><b><i>Dec '21</b></td>
    <td width="84%" valign="top" align="left" style="padding-left: 10px;">
      Proposed my PhD thesis titled <a href="https://www.cs.cmu.edu/calendar/wed-2021-12-01-1030/machine-learning-thesis-proposal" target="_blank"><i>Active robot perception using programmable light curtains</i></a>. Expected to defend and graduate in July 2022. Thesis committee: <a href="https://davheld.github.io/" target="_blank">David Held</a>, <a href="https://www.cs.cmu.edu/~srinivas/" target="_blank">Srinivasa Narasimhan</a>, <a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a> and <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Wolfram Burgard</a>.
    </td>
  </tr>
  <tr>
    <td width="16%" valign="top" align="right"><b><i>Nov '21</b></td>
    <td width="84%" valign="top" align="left" style="padding-left: 10px;">
      Published a <a href="https://blog.ml.cmu.edu/2021/11/19/active-safety-envelopes-using-light-curtains-with-probabilistic-guarantees/" target="_blank">CMU ML Blog Post</a> on our recent work on safety envelopes using light curtains, presented at RSS '21. Check it out!
    </td>
  </tr>
</table><hr><br> -->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications </sectionheading>(representative papers are <span class="highlight">highlighted</span>)</td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <!-- CVPR 2022 Paper -->
  <tr class="pub" bgcolor="#ffffd0">
    <td class="pub_thumbnail">
      <a href="https://tien-d.github.io/egodepthnormal_cvpr22.html" target="_blank">
        <img class="pub" src="data/projects/egodepth_cvpr22/egodepth_demo.png" alt="image not found" width="93%"></a></td>
      </a>
    </td>
    <td class="pub_text">
      <p><a href="data/projects/egodepth_cvpr22/egodepth_cvpr22_final.pdf" target="_blank">      
      <heading>Egocentric Scene Understanding via Multimodal Spatial Rectifier</heading></a><br>


      <a href=https://tien-d.github.io/ target="_blank"> Tien Do</a>, 
      <u><strong>Khiem Vuong</strong></u>, 
      <a href=https://www-users.cs.umn.edu/~hspark/ target="_blank"> Hyun Soo Park</a> <br>
      
      <span style="color:darkred"><b>CVPR 2022</b></span> <span style="color:red">(Oral presentation)</span>
      </p>

      <div>
      <a href="https://tien-d.github.io/egodepthnormal_cvpr22.html" target="_blank">webpage</a> |
      <a href="javascript:toggleblock('cvpr22_abs')">abstract</a> |
      <a href="data/projects/egodepth_cvpr22/egodepth_cvpr22_final.pdf" target="_blank">pdf</a> |
      <a href="https://github.com/tien-d/EgoDepthNormal" target="_blank">code</a> |
      <a href="https://www.youtube.com/watch?v=bLp9jYLQD-Q" target="_blank">video</a> |
      <a shape="rect" href="javascript:togglebib('cvpr22')" class="togglebib">bibtex</a> 

      <p align="justify"> <i id="cvpr22_abs">In this paper, we study a problem of egocentric scene understanding, i.e., predicting depths and surface normals from an egocentric image. Egocentric scene understanding poses unprecedented challenges: (1) due to large head movements, the images are taken from non-canonical viewpoints (i.e., tilted images) where existing models of geometry prediction do not apply; (2) dynamic foreground objects including hands constitute a large proportion of visual scenes. These challenges limit the performance of the existing models learned from large indoor datasets, such as ScanNet and NYUv2, which comprise predominantly upright images of static scenes. We present a multimodal spatial rectifier that stabilizes the egocentric images to a set of reference directions, which allows learning a coherent visual representation. Unlike unimodal spatial rectifier that often produces excessive perspective warp for egocentric images, the multimodal spatial rectifier learns from multiple directions that can minimize the impact of the perspective warp. To learn visual representations of the dynamic foreground objects, we present a new dataset called EDINA (Egocentric Depth on everyday INdoor Activities) that comprises more than 500K synchronized RGBD frames and gravity directions. Equipped with the multimodal spatial rectifier and the EDINA dataset, our proposed method on single-view depth and surface normal estimation significantly outperforms the baselines not only on our EDINA dataset, but also on other popular egocentric datasets, such as First Person Hand Action (FPHA) and EPIC-KITCHEN.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('cvpr22_abs');
      </script>
      </div>

      <p>
        Extension of the spatial rectifier to the multi-directional case, applying to depth/surface normal prediction from egocentric view, accompanied by a novel dataset to facilitate learning this visual representation.
      </p>


    </td>
  </tr>

  <tr>
    <td colspan="2" style="padding: 0px">
      <div class="paper" id="cvpr22">
        <pre xml:space="preserve">
          @InProceedings{Do_2022_EgoSceneMSR,
            author     = {Do, Tien and Vuong, Khiem and Park, Hyun Soo},
            title      = {Egocentric Scene Understanding via Multimodal Spatial Rectifier},
            booktitle  = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
            month      = {June},
            year       = {2022}}
          </pre>
      </div>
    </td>
  </tr>


  <tr class="pub">
    <td class="pub_thumbnail">
      <a href="https://github.com/MARSLab-UMN/DeepMultiviewDepth" target="_blank">
        <img class="pub" src="data/projects/depthuncertainty_icra21/icra2021.gif" alt="image not found" width="93%"></a></td>
      </a>
    </td>
    <td class="pub_text">
      <p><a href="https://arxiv.org/abs/2011.09594" target="_blank">
      <heading>Deep Multi-view Depth Estimation with Predicted Uncertainty</heading></a><br>
            
      <a href=https://scholar.google.com/citations?user=snbAqkwAAAAJ&hl=en target="_blank"> Tong Ke</a>,
      <a href=https://tien-d.github.io/ target="_blank"> Tien Do</a>, 
      <u><strong>Khiem Vuong</strong></u>,
      <a href=https://scholar.google.com/citations?user=WBZAFHAAAAAJ&hl=en target="_blank"> Kourosh Sartipi</a>,
      <a href=https://www-users.cs.umn.edu/~stergios/index.html target="_blank"> Stergios I. Roumeliotis</a> <br>

      <span style="color:darkred"><b>ICRA 2021</b></span>
      </p>

      <div>
      <!-- <a href="https://www.khiemvuong.com/TiltedImageSurfaceNormal/" target="_blank">webpage</a> | -->
      <a href="javascript:toggleblock('icra21_abs')">abstract</a> |
      <a href="https://arxiv.org/abs/2011.09594" target="_blank">pdf</a> |
      <a href="https://github.com/MARSLab-UMN/DeepMultiviewDepth" target="_blank">code</a> |
      <!-- <a href="https://youtu.be/3TswQYOK9N0" target="_blank">video</a> | -->
      <a shape="rect" href="javascript:togglebib('icra21')" class="togglebib">bibtex</a> 
      <!-- <a href="https://blog.ml.cmu.edu/2021/11/19/active-safety-envelopes-using-light-curtains-with-probabilistic-guarantees/" target="_blank">blog</a> -->


      <p align="justify"> <i id="icra21_abs">In this paper, we address the problem of estimating dense depth from a sequence of images using deep neural networks. Specifically, we employ a dense-optical-flow network to compute correspondences and then triangulate the point cloud to obtain an initial depth map.Parts of the point cloud, however, may be less accurate than others due to lack of common observations or small parallax. To further increase the triangulation accuracy, we introduce a depth-refinement network (DRN) that optimizes the initial depth map based on the image's contextual cues. In particular, the DRN contains an iterative refinement module (IRM) that improves the depth accuracy over iterations by refining the deep features. Lastly, the DRN also predicts the uncertainty in the refined depths, which is desirable in applications such as measurement selection for scene reconstruction. We show experimentally that our algorithm outperforms state-of-the-art approaches in terms of depth accuracy, and verify that our predicted uncertainty is highly correlated to the actual depth error.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('icra21_abs');
      </script>
      </div>

      Depth estimation by multiview triangulation, followed by an iterative depth refinement module that preserves estimates with high triangulation confidence.

    </td>
  </tr>

  <tr>
    <td colspan="2" style="padding: 0px">
      <div class="paper" id="icra21">
        <pre xml:space="preserve">
          @inproceedings{ke2021deep,
            title={Deep Multi-view Depth Estimation with Predicted Uncertainty},
            author={Ke, Tong and Do, Tien and Vuong, Khiem and Sartipi, Kourosh and Roumeliotis, Stergios I},
            booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
            pages={9235--9241},
            year={2021},
            organization={IEEE}
          }
          </pre>
      </div>
    </td>
  </tr>


  <!-- ECCV 2020 Paper -->
  <tr class="pub" bgcolor="#ffffd0">
    <td class="pub_thumbnail">
      <a href="https://github.com/MARSLab-UMN/TiltedImageSurfaceNormal" target="_blank">
        <img class="pub" src="data/projects/tiltednormal_eccv20/surface_normal.gif" alt="image not found" width="93%"></a></td>
      </a>
    </td>
    <td class="pub_text">
      <p><a href="https://arxiv.org/abs/2007.09264" target="_blank">
      <heading>Surface Normal Estimation of Tilted Images via Spatial Rectifier</heading></a><br>
      
      <a href=https://tien-d.github.io/ target="_blank"> Tien Do</a>, 
      <u><strong>Khiem Vuong</strong></u>, 
      <a href=https://www-users.cs.umn.edu/~stergios/index.html target="_blank"> Stergios I. Roumeliotis</a>,
      <a href=https://www-users.cs.umn.edu/~hspark/ target="_blank"> Hyun Soo Park</a> <br>
      
      <span style="color:darkred"><b>ECCV 2020</b></span> <span style="color:red">(Spotlight presentation)</span>
      </p>

      <div>
      <a href="https://www.khiemvuong.com/TiltedImageSurfaceNormal/" target="_blank">webpage</a> |
      <a href="javascript:toggleblock('eccv20_abs')">abstract</a> |
      <a href="https://arxiv.org/abs/2007.09264" target="_blank">pdf</a> |
      <a href="https://github.com/MARSLab-UMN/TiltedImageSurfaceNormal" target="_blank">code</a> |
      <a href="https://youtu.be/3TswQYOK9N0" target="_blank">video</a> |
      <a shape="rect" href="javascript:togglebib('eccv20')" class="togglebib">bibtex</a> 

      <p align="justify"> <i id="eccv20_abs">In this paper, we present a spatial rectifier to estimate surface normals of tilted images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. Our two main hypotheses are: (1) visual scene layout is indicative of the gravity direction; and (2) not all surfaces are equally represented by a learned estimator due to the structured distribution of the training data, thus, there exists a transformation for each tilted image that is more responsive to the learned estimator than others. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. Along with the spatial rectifier, we propose a novel truncated angular loss that offers a stronger gradient at smaller angular errors and robustness to outliers. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('eccv20_abs');
      </script>
      </div>

      Robust surface normal estimation by spatially rectifying image to the densely distributed orientations.

    </td>
  </tr>

  <tr>
    <td colspan="2" style="padding: 0px">
      <div class="paper" id="eccv20">
        <pre xml:space="preserve">
          @InProceedings{Do2020SurfaceNormal,
            author = {Do, Tien and Vuong, Khiem and Roumeliotis, Stergios I. and Park, Hyun Soo},
            title = {Surface Normal Estimation of Tilted Images
            via Spatial Rectifier},
            booktitle = {Proc. of the European Conference on Computer Vision},
            month = {August} # { 23--28},
            address={Virtual Conference},
            year = {2020}}
          </pre>
      </div>
    </td>
  </tr>


  <tr class="pub">
    <td class="pub_thumbnail">
      <a href="https://github.com/MARSLab-UMN/vi_depth_completion" target="_blank">
        <img class="pub" src="data/projects/deepvislam_iros20/iros2020.gif" alt="image not found" width="93%"></a></td>
      </a>
    </td>
    <td class="pub_text">
      <p><a href="https://arxiv.org/pdf/2008.00092.pdf" target="_blank">
      <heading>Deep Depth Estimation from Visual-Inertial SLAM</heading></a><br>
            
      <a href=https://scholar.google.com/citations?user=WBZAFHAAAAAJ&hl=en target="_blank"> Kourosh Sartipi</a>,
      <a href=https://tien-d.github.io/ target="_blank"> Tien Do</a>, 
      <a href=https://scholar.google.com/citations?user=snbAqkwAAAAJ&hl=en target="_blank"> Tong Ke</a>,
      <u><strong>Khiem Vuong</strong></u>, 
      <a href=https://www-users.cs.umn.edu/~stergios/index.html target="_blank"> Stergios I. Roumeliotis</a>

      <span style="color:darkred"><b>IROS 2020</b></span>
      </p>

      <div>
      <!-- <a href="https://www.khiemvuong.com/TiltedImageSurfaceNormal/" target="_blank">webpage</a> | -->
      <a href="javascript:toggleblock('iros20_abs')">abstract</a> |
      <a href="https://arxiv.org/pdf/2008.00092.pdf" target="_blank">pdf</a> |
      <a href="https://github.com/MARSLab-UMN/vi_depth_completion" target="_blank">code</a> |
      <!-- <a href="https://youtu.be/3TswQYOK9N0" target="_blank">video</a> | -->
      <a shape="rect" href="javascript:togglebib('iros20')" class="togglebib">bibtex</a> 
      <!-- <a href="https://blog.ml.cmu.edu/2021/11/19/active-safety-envelopes-using-light-curtains-with-probabilistic-guarantees/" target="_blank">blog</a> -->


      <p align="justify"> <i id="iros20_abs">This paper addresses the problem of learning to complete a scene's depth from sparse depth points and images of indoor scenes. Specifically, we study the case in which the sparse depth is computed from a visual-inertial simultaneous localization and mapping (VI-SLAM) system. The resulting point cloud has low density, it is noisy, and has non-uniform spatial distribution, as compared to the input from active depth sensors, e.g., LiDAR or Kinect. Since the VI-SLAM produces point clouds only over textured areas, we compensate for the missing depth of the low-texture surfaces by leveraging their planar structures and their surface normals which is an important intermediate representation. The pre-trained surface normal network, however, suffers from large performance degradation when there is a significant difference in the viewing direction (especially the roll angle) of the test image as compared to the trained ones. To address this limitation, we use the available gravity estimate from the VI-SLAM to warp the input image to the orientation prevailing in the training dataset. This results in a significant performance gain for the surface normal estimate, and thus the dense depth estimates. Finally, we show that our method outperforms other state-of-the-art approaches both on training (ScanNet and NYUv2) and testing (collected with Azure Kinect) datasets.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('iros20_abs');
      </script>
      </div>

      Depth estimation by completing a very sparse VI-SLAM point cloud using planar constraint from robust surface normal prediction.

    </td>
  </tr>

  <tr>
    <td colspan="2" style="padding: 0px">
      <div class="paper" id="iros20">
        <pre xml:space="preserve">
          @inproceedings{sartipi2020deep,
            title={Deep depth estimation from visual-inertial slam},
            author={Sartipi, Kourosh and Do, Tien and Ke, Tong and Vuong, Khiem and Roumeliotis, Stergios I},
            booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
            pages={10038--10045},
            year={2020},
            organization={IEEE}
          }
          </pre>
      </div>
    </td>
  </tr>

</table><hr>

<!--Jobs-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Work Experience</sectionheading></td></tr>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tr>
    <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
      <img src="data/images/companies/ri_logo.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
    </td>

    <td class="exp-description">
      <b>Research Assistant</b>
      <span style="float: right;">Oct. 2021 - Present</span>
      <br>
      <a href="http://www.cs.cmu.edu/~ILIM" target="_blank">Illumination and Imaging Laboratory (ILIM)</a>, <a href='https://imaging.cs.cmu.edu/' target="_blank">Imaging Group</a>
      <br>
      Advisor: Prof. <a href='http://www.cs.cmu.edu/~srinivas/' target="_blank">Srinivasa Narasimhan</a></a>
    </td>
  </tr>


  <tr>
    <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
      <img src="data/images/companies/marslab.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
    </td>

    <td class="exp-description">
      <b>Research Assistant</b>
      <span style="float: right;">Sep. 2019 - Jun. 2021</span>
      <br>
      <a href="http://mars.cs.umn.edu/" target="_blank">Multiple Autonomous Robotic Systems Laboratory (MARS)</a>
      <br>
      Advisor: Prof. <a href="https://www-users.cs.umn.edu/~stergios/index.html" target="_blank">Stergios I. Roumeliotis</a> & Prof. <a href="https://www-users.cs.umn.edu/~hspark/" target="_blank">Hyun Soo Park</a>
    </td>
  </tr>

  <tr>
    <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
      <img src="data/images/companies/enfusion.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
    </td>

    <td class="exp-description">
      <b>Software Engineering Intern</b>
      <span style="float: right;">Jun. 2019 - Aug. 2019</span>
      <br>
      <a href="https://www.enfusionsystems.com/" target="_blank">Enfusion Systems</a>
      <br>
      Team: Backend services, Portfolio Management Systems (PMS)
    </td>
  </tr>

</table>




  <!--Jobs-->
<!-- <table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
    <li> Google Faculty Research Award (2020)</li>
    <li> Facebook Graduate Fellowship (2018-2020)</li>
    <li> Nvidia Graduate Fellowship (2017-2018)</li>
    <li> Snapchat Inc. Graduate Fellowship (2017)</li>
    <li> Gold Medal in Computer Science at IIT Kanpur (2014)</li>
    <li> Best Undergraduate Thesis Award at IIT Kanpur (2014)</li>
    </ul>
  </td></tr>
</table> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="2.0">
    Modified version of template from <a href="https://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>

</html>
